```{r}

#Importing Library
library(deepnet)

#Loading raw data and titling columns 
raw_data <- read.csv("/Users/Rahul Behal/OneDrive/Desktop/Deep Learning with MQL/EURUSD15.csv")

data = raw_data

names(data) <- c("Open","High","Low","Close")

Open = data["Open"]
High = data["High"]
Low = data["Low"]
Close = data["Close"]


```

```{r}

#Creating function that creates a middle of HL value and a price change value
OHLC <- function (o,h,l,c)
  
{
 
price <- cbind(Open = rev(o), High = rev(h), Low = rev(l), Close = rev(c))
Med <- (price[, 2] + price[, 3])/2
CO <- price[, 4] - price[, 1]

price <<- cbind(price, Med, CO)

head(price)
 
}

OHLC(Open,High,Low,Close)
```
```{r}
#Welles Wilder's Directional Movement Index
library(TTR)
adx <- ADX(price, n=16)
plot.ts(head(adx,200))

```
```{r}
summary(adx)
```
```{r}
#Aroon
ar <- aroon(price[, c('High', 'Low')], n=16)[,'oscillator']
plot(head(ar,200), type="l")
abline(h=0)
```

```{r}
summary(ar)
```

```{r}
#Commodity Channel Index
cci <- CCI(price[, 2:4], n=16)
plot.ts(head(cci,200))
abline(h=0)
```

```{r}
summary(cci)
```
```{r}
#Chaikin Volatility
chv <- chaikinVolatility(price[,2:4],n=16)
summary(chv)
```
```{r}
plot(head(chv,200),t="l")
abline(h=0)
```
```{r}
#Chande Momemtum Oscillator
cmo <- CMO(price[,'Med'],n=16)
plot(head(cmo,200),t="l")
abline(h=0)
```

```{r}
summary(cmo)
```

```{r}
#MACD Oscillator
macd <- MACD(price[,'Med'], 12, 26, 9)[,'macd']
plot(head(macd,200),t="l")
abline(h=0)
```
```{r}
summary(macd)
```
```{r}
#Oscillator of a Moving Average
osma <-macd - MACD(price[,'Med'],12,26,9)[, "signal"]
plot(head(osma,200),t="l")
abline(h=0)
```
```{r}
summary(osma)
```

```{r}
#Relative Strength Index
rsi <- RSI(price[,'Med'],n=16)
plot(head(rsi,200), t="l")
abline(h=50)
```
```{r}
summary(rsi)
```

```{r}
#Stochastic Oscillator
stoh <- stoch(price[,2:4],14,3,3)
plot.ts(head(stoh,200))
```
```{r}
summary(stoh)
```

```{r}
#Stochastic Momentum Index
smi <- SMI(price[,2:4],n=13,nFast=2,nSlow=25,nSig=9)
plot.ts(head(smi,200))
```
```{r}
summary(smi)
```

```{r}
#Volatility (Yang and Zhang)
vol <- volatility(price[, 1:4],n=16,calc = "yang.zhang", N=96)
plot.ts(head(vol,200))
```

```{r}
summary(vol)
```

```{r}
#Preparing the raw input data
In <- function(p=16){
  
  adx<-ADX(price,n=p);
  ar<-aroon(price[ ,c('High','Low')],n=p)[,'oscillator'];
  cci<-CCI(price[, 2:4], n=p);
  chv<-chaikinVolatility(price[ ,2:4],n=p);
  cmo<-CMO(price[ ,'Med'], n=p);
  macd<-MACD(price[ ,'Med'], 12, 26, 9)[ , 'macd'];
  osma<-macd - MACD(price[ ,'Med'], 12, 26, 9)[, 'signal'];
  stoh<-stoch(price[ ,2:4], 14, 3, 3);
  smi<-SMI(price[ ,2:4],n=p,nFast=2,nSlow=25,nSig=9);
  vol<-volatility(price[, 1:4],n=p,calc="yang.zhang", N=96)
  In<-cbind(adx,ar,cci,chv,cmo,macd,osma,rsi,stoh,smi,vol);
  return(In)
  
}

X <- In()

tail(X)
  
```
```{r}
#Preparing the output data

Out <- function(ch=0.0037){
 
#Creating ZigZag
  zz <- ZigZag(price[ ,'Med'], change = ch, percent = F, retrace = F, lastExtreme = T)
 
#Returning all unknown values to the last known value
  n <- 1:length(zz);
  
  for(i in n){
   if(is.na(zz[i]))
      zz[i] = zz[i-1];
  }

#Defining the 'speed' of ZigZag changes 
  speed <- c(diff(zz), NA);

#Defining the signal
  signal <- ifelse(speed>0, 0, ifelse(speed<0, 1, NA));

return(signal)
  
}

Y <- Out()

table(Y)
```
```{r}
#Writing a function to clear data of undefined, autocorrelated, and almost 0 variables

Clearing <- function(x,y){
  
  data <- cbind(x,y);
  n <- ncol(data)
  data <- na.omit(data)
  return(data);
  
}

#Clearing dataset

data <- Clearing(X,Y);

head(data)
nrow(data)

```
```{r}
test = table(data[ ,ncol(data)])
test
```


```{r}
#Writing a function that balances the data so there's an equal number of each output

Balancing <- function(data){
  
  #Table with number of classes
  class <- table(data[ ,ncol(data)]);
  #If the divergence is less than 15%, return the initial matrix
  if(max(class)/min(class) <= 1.15)
    return(data)
  #otherwise level by the greater side
  data <- if(max(class)/min(class) > 1.15){
            upSample(x = data[ ,-ncol(data)], y = as.factor(data[, ncol(data)]), yname = "Y")
          }
  
  #Convert y (factor) into a number
  data$Y <- as.numeric(data$Y)
  
  #Recode y from 1,2 into 0,1
  data$Y <- ifelse(data$Y==1 , 0 , 1)
  
  #Convert dataframe to matrix
  data <- as.matrix(data)
  return(data);
  
}

#Balancing Data

data_bal <- Balancing(data)

x <- data_bal[ , -ncol(data_bal)]
y <- data_bal[ , ncol(data_bal)]

```

```{r}
library('rminer')
library('ggplot2')
library('lattice')
library('caret')


#Creating train and test data
t <- holdout(y, ratio = 8/10, mode = "random")

#Preprocessing data

spSign <- preProcess(x[t$tr, ], method = "spatialSign")
x.tr <- predict(spSign, x[t$tr, ])
x.ts <- predict(spSign, x[t$ts, ])

```

```{r}
#Building deep neural network stacked autoencoder

system.time(
  
  SAE <- sae.dnn.train(x= x.tr, y= y[t$tr], hidden=c(100,100,100), activationfun = "tanh", 
                       learningrate = 0.6, momentum = 0.5, learningrate_scale = 1.0, output = "sigm", 
                       sae_output = "linear", numepochs = 10, batchsize = 100, hidden_dropout = 0, visible_dropout = 0
                       )
  
    )

```
```{r}
#Evaluating Forecasts
pr.sae <- nn.predict(SAE, x.ts);
summary(pr.sae)
```

```{r}
#Converting values to either 0 or 1 
pr <- ifelse(pr.sae>mean(pr.sae), 1, 0)

#Converting into factors
pr = as.factor(pr)
testy = as.factor(y[t$ts])

#Creating Confusion Matrix
confusionMatrix(testy, pr)
```
```{r}
#Testing on the last 500 bars

#Creating new data
new.x <- predict(spSign, tail(data[,-ncol(data)], 500))

#Predicting new data and creating signals, -1 for sell, 1 for buy
pr.sae1 <- nn.predict(SAE, new.x)
pr.signal  <- ifelse(pr.sae1>mean(pr.sae1),-1,1)

table(pr.signal)
```
```{r}
#Creating new y
new.y <- ifelse(tail(data[,ncol(data)], 500) == 0, 1, -1)

table(new.y)
```
```{r}
factorize <- function(obj){
  
  objfactored = as.factor(obj)
  
  return(objfactored)
}


library('caret')
cm1 <- confusionMatrix(factorize(new.y),factorize(pr.signal))
cm1
```

```{r}
#Testing profit for the last 500 bars using predicted signals

#calculating sum of signal times close-open profit

pr.signal = as.numeric(pr.signal)
bal <- cumsum(tail(price[ ,'CO'], 500)*pr.signal)

plot(bal, t="l")
abline(h=0)
```

```{r}
#Comparing with the balance that would have been obtained from the ideal signals of ZZ

new.y = as.numeric(new.y)
bal.zz<-cumsum(tail(price[ , 'CO'], 500) * new.y)
plot(bal.zz,  t = "l")

#The red line is the balance by the neural network
lines(bal,  col = 2)
```
```{r}
#Building Estimation() function, this will generate coefficients Accuracy/Err
#This allows us to obtain a result right away changing some parameters and seeing what influences network



########################################################################
#############
#Parameters:#
#############
#X - Matrix of input raw predictors;
#Y - vector of the target variable;
#r - ratio train/test;
#m - sample formation mode (random or consequent);
#norm - mode of input parameters normalizing ([ -1, 1]= "spatialSign";[0, #1]="range");
#h - vector with a number of neurons in the hidden layers;
#act - activation function of hidden neurons;
#LR - training level;
#?????? - momentum;
#out - activation function of the output layer;
#sae - activation function of the autoencoder;
#Ep - number of the training epochs;
#Bs - size of the small sample;
#??M- Boolean variable , if TRUE print Accuracy. Else Err.
#######################################################################



Estimation <- function(X, Y, r=8/10, m="random",norm="spatialSign",h=c(10),act="tanh", LR=0.8,Mom=0.5,out="sigm",sae="linear",Ep=10,Bs=50,CM=F){
  
    #Indices of the training and test data set
    t <- holdout(Y, ratio=r, mode=m)
    
    #Parameters of preproceessing
    prepr <- preProcess(X[t$tr, ],method=norm)
    
    #Divide into train and test datasets with preprocessing
    x.tr <- predict(prepr, X[t$tr, ])
    x.ts <- predict(prepr, X[t$ts, ])
    
    #Training the model
    y.tr <- Y[t$tr]; y.ts <- Y[t$ts]
    SAE <- sae.dnn.train(x=x.tr, y=y.tr, hidden=h, activationfun=act, 
                     learningrate=LR, momentum=Mom, output=out,
                     sae_output=sae, numepochs=Ep, batchsize=Bs)

    #Obtain a forecast on the test data sets
    pr.sae <- nn.predict(SAE, x.ts)
    
    #Recode it into signals 1,0
    pr <- ifelse(pr.sae>mean(pr.sae),1,0)
    
    #Calculate the accuracy coefficient or classification error
    if(CM)
      err <- unname(confusionMatrix(y.ts,pr)$overall[1])

    if(!CM)
      
      err <- nn.test(SAE, x.ts, y.ts, mean(pr.sae))
return(err)
    
}
```



```{r}
#Calculating classification error on unbalanced dataset dt 
  #by network with 3 hidden layers each with 30 neurons

#Error function
Err <- Estimation(X=data[,-ncol(data)], Y=data[ ,ncol(data)], h=c(30, 30, 30), LR=0.7)
```

```{r}
#Error

Err
```

```{r}
#Testing function that calculates balance by the forecast signals or by the ideal ones 

########################################################################
#############
#Parameters:#
#############
#dt1 - matrix of the input and target variable used for training the network

#dt2 - matrix of the input and target variables used for testing the network

#pr - Boolean variable, if TRUE print the balance by the forecast signals, otherwise by ZigZag

#bar - the number of the last bars to use for calculating balance
#######################################################################

Testing <- function(dt1, dt2, r=8/10, m="random", norm="spatialSign", 
                    h=c(10), act="tanh", LR=0.8, Mom=0.5, out="sigm", 
                    sae="linear", Ep=10, Bs=50, pr=T, bar=500){
  
  X <- dt1[ ,-ncol(dt1)]
  Y <- dt1[ ,ncol(dt1)]
  
  t <- holdout(Y, ratio=r, mode=m)
  
  prepr <- preProcess(X[t$tr, ], method=norm)
  
  x.tr <- predict(prepr, X[t$tr, ])
  y.tr <- Y[t$tr];
  
  SAE <- sae.dnn.train(x=x.tr, y=y.tr, hidden=h, activationfun=act, 
                       learningrate=LR, momentum=Mom, output=out, 
                       sae_output=sae, numepochs=Ep, batchsize=Bs)
  
  X <- dt2[ ,-ncol(dt2)]
  Y <- dt2[ ,ncol(dt2)]
  
  x.ts <- predict(prepr, tail(X, bar))
  y.ts <- tail(Y, bar)
  
  pr.sae <- nn.predict(SAE, x.ts)
  
  sig <- ifelse(pr.sae>mean(pr.sae), -1, 1)
  sig.zz <- ifelse(y.ts==0, 1, -1)
  
  bal <- cumsum(tail(price[ ,'CO'], bar)*sig)
  bal.zz <- cumsum(tail(price[ ,'CO'], bar)*sig.zz)
    if(pr)
      return(bal)
  
  if(!pr)
    return(bal.zz)
  
}
```

```{r}
#Calculating balance o nthe last 500 bars of our dataset when training 
#on the balanced data set by the neural networks with the same parameters

Bal <- Testing(data_bal, data, h=c(30,30,30), LR=0.7)
```
```{r}
#Plotting

plot(Bal, t='l')
abline(h=0)
```

```{r}
Testing.1 <- function(dt1, dt2, r=8/10, m="random", norm="spatialSign", 
                    h=c(10), act="tanh", LR=0.8, Mom=0.5, out="sigm", 
                    sae="linear", Ep =10, Bs=50, pr=T, bar=500, dec=1){
  
  X <- dt1[ ,-ncol(dt1)]
  Y <- dt1[ ,ncol(dt1)]
  
  t <- holdout(Y, ratio=r, mode=m)
  
  prepr <- preProcess(X[t$tr, ], method=norm)
  
  x.tr <- predict(prepr, X[t$tr, ])
  y.tr <- Y[t$tr];
  
  SAE <- sae.dnn.train(x=x.tr, y=y.tr, hidden=h, activationfun=act, 
                       learningrate=LR, momentum=Mom, output=out, 
                       sae_output=sae, numepochs=Ep, batchsize=Bs)
  
  X <- dt2[ ,-ncol(dt2)]
  Y <- dt2[ ,ncol(dt2)]
  
  x.ts <- predict(prepr, tail(X, bar))
  y.ts <- tail(Y, bar)
  
  pr.sae <- nn.predict(SAE, x.ts)
  
  #+/- mean
  if(dec==1)
      sig <- ifelse(pr.sae>mean(pr.sae),-1,1)
  
  #60/40
  if(dec==2)
      sig <- ifelse(pr.sae>0.6,-1,ifelse(pr.sae<0.4,1,0))
  
  
  sig.zz <- ifelse(y.ts==0, 1, -1)
  
  bal <- cumsum(tail(price[ ,'CO'], bar)*sig)
  bal.zz <- cumsum(tail(price[ ,'CO'], bar)*sig.zz)
  
  if(pr)
    return(bal)
  
  if(!pr)
    return(bal.zz)
  
}
```

```{r}
#In order to repeat results, setting pseudorandom NG
set.seed <- 1245

Bal1 <- Testing.1(data_bal, data, h=c(30,30,30), LR=0.7, dec=1)
```
```{r}
set.seed <- 1245

Bal2 <- Testing.1(data_bal, data, h=c(30,30,30), LR=0.7, dec=2)
```

```{r}
plot(Bal2, t="l")
lines(Bal1, col=2)
```


```{r}
Testing.2 <- function(dt1, dt2, r=8/10, m="random", norm="spatialSign", 
                    h=c(10), act="tanh", LR=0.8, Mom=0.5, out="sigm", 
                    sae="linear", Ep=10, Bs=50, pr=T, bar=500, dec=1,
                    ans=1){
  
  


  #Features and targets
  X <- dt1[ ,-ncol(dt1)]
  Y <- dt1[ ,ncol(dt1)]
  
  #Splitting dataset 
  t <- holdout(Y, ratio=r, mode=m)
  
  #Preprocessing
  prepr <<- preProcess(X[t$tr, ], method=norm)
  
  #Training data
  x.tr <- predict(prepr, X[t$tr, ])
  y.tr <- Y[t$tr];
  
#where times(ans) is a number of networks that we want to obtain
#.packages points to the package to take the calculated function. 
#The result has a form of a list and contains the number of trained networks we need.

#Parallel computing
library(doParallel)
library(foreach)
puskCluster <- function() {
  cores <- detectCores()
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)
  clusterSetRNGStream(cl)
  return(cl)
}

    
  
#Training models
cl <- puskCluster()
SAE <- foreach(times(ans), .packages = "deepnet") %dopar%
                    sae.dnn.train(x=x.tr, y=y.tr, hidden=h, learningrate=LR, 
                                  activationfun=act,momentum=Mom, output=out, 
                                  sae_output=sae,numepochs=Ep, batchsize=Bs)
  

  stopCluster(cl) 
  #Testing sets
  X <- dt2[ ,-ncol(dt2)]
  Y <- dt2[ ,ncol(dt2)]
  
  x.ts <- predict(prepr, tail(X, bar))
  y.ts <- tail(Y, bar)
  
  
  #Forecast from every network and calculating the average
  pr.sae <- (foreach(i=1:ans, .combine="+") %do% nn.predict(SAE[[i]], x.ts))/ans
  

  
  #Here i is a vector of the trained network indices
  #.combine="+" specifies what form the returned predicts in all neural networks are supposed to be returned in. 
  #In this case we required to return a sum and perform these computations sequentially, not in a parallel way (operator %do%). 
  #The obtained sum is going to be divided by the number of the neural networks and it will be the end result.
  
  #+/- mean
  if(dec==1)
      sig <- ifelse(pr.sae>mean(pr.sae),-1,1)
  
  #60/40
  if(dec==2)
      sig <- ifelse(pr.sae>0.6,-1,ifelse(pr.sae<0.4,1,0))
  
  
  sig.zz <- ifelse(y.ts==0, 1, -1)
  
  bal <- cumsum(tail(price[ ,'CO'], bar)*sig)
  bal.zz <- cumsum(tail(price[ ,'CO'], bar)*sig.zz)
  
  if(pr)
    #print(bal)    return(bal)
  
  if(!pr)
    #print(bal.zz)
    return(bal.zz)

  
}
```


```{r}
#Creating and saving a model

system.time(bal<-Testing.2(data_bal, data, h = c(50, 50, 50), LR = 0.7, dec = 2, Ep=500, bar=500))
```

```{r}
plot(bal, t="l")
```

```{r}
#Saving nerual network under different name
SAE1 <- SAE

system.time(bal <- Testing.2(data_bal, data, h=c(50,50,50), LR=0.7, 
                             dec=2, Ep=300, bar=500))

```

```{r}
plot(bal, t="l")
```
```{r}
#Saving model

#save(SAE, prepr, file="C:/Users/Rahul Behal/OneDrive/Desktop/Deep Learning with MQL/SAE.model") 

ls()
```
```{r}
save(SAE, prepr, file="C:/Users/Rahul Behal/OneDrive/Desktop/Deep Learning with MQL/SAE.model") 

plot(bal, t="l")

```

